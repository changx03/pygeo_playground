{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision.datasets import Omniglot\n",
    "from torchvision.transforms.functional import to_pil_image\n",
    "import numpy as np\n",
    "import pytorch_lightning as pl\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as T\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Siamese(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Siamese, self).__init__()\n",
    "\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(1, 64, 7),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2),\n",
    "            nn.Conv2d(64, 128, 5),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2),\n",
    "            nn.Conv2d(128, 128, 5),\n",
    "            nn.ReLU(),\n",
    "            nn.Flatten(1)\n",
    "        )\n",
    "        self.linear1 = nn.Linear(8192, 4096)\n",
    "        self.linear2 = nn.Linear(4096, 1)\n",
    "\n",
    "    def forward_one(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        x = self.conv(x)\n",
    "        x = self.linear1(x)\n",
    "\n",
    "        x = F.sigmoid(x)\n",
    "        return x\n",
    "    \n",
    "    def forward(self, x1: torch.Tensor, x2: torch.Tensor) -> torch.Tensor:\n",
    "        x1 = self.forward_one(x1)\n",
    "        x2 = self.forward_one(x2)\n",
    "        dist = torch.abs(x1 - x2)\n",
    "        out = self.linear2(dist)\n",
    "        return out\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For testing outputs\n",
    "\n",
    "# net = Siamese()\n",
    "# # Samples in Omniglot is [1, 105, 105]\n",
    "# x1 = torch.rand([1, 1, 64, 64])\n",
    "# out = net.forward_one(x1)\n",
    "# print(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 96\n",
    "NUM_WORKERS = 16\n",
    "RESIZE = (64, 64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "# of training samples: 19280\n",
      "# of test samples:     13180\n"
     ]
    }
   ],
   "source": [
    "transform_train = T.Compose([T.RandomResizedCrop(RESIZE, scale=(0.6, 1.0)), T.RandomAffine(15, fill=1.), T.ToTensor()])\n",
    "transform_test = T.Compose([T.Resize(RESIZE), T.ToTensor()])\n",
    "\n",
    "omniglot_bg = Omniglot(root='./data/omniglot', background=True, transform=transform_train, download=True)\n",
    "omniglot_eval = Omniglot(root='./data/omniglot', background=False, transform=transform_test, download=True)\n",
    "\n",
    "print(f'# of training samples: {len(omniglot_bg)}')\n",
    "print(f'# of test samples:     {len(omniglot_eval)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def show(imgs):\n",
    "    \"\"\"Show images\"\"\"\n",
    "    if not isinstance(imgs, list):\n",
    "        imgs = [imgs]\n",
    "    fig, axs = plt.subplots(ncols=len(imgs), squeeze=False)\n",
    "    for i, img in enumerate(imgs):\n",
    "        img = img.detach()\n",
    "        img = to_pil_image(img)\n",
    "        axs[0, i].imshow(np.asarray(img), cmap='gray', vmin=0, vmax=1)\n",
    "        axs[0, i].set(xticklabels=[], yticklabels=[], xticks=[], yticks=[])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "torch.Size([1, 105, 105])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lukec/workspace/pygeo_playground/venv/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 5 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 5 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "raw_data = Omniglot(root='./data/omniglot', background=False, transform=T.Compose([T.ToTensor()]), download=True)\n",
    "img = raw_data[0][0]\n",
    "print(img.size())\n",
    "images = [img]\n",
    "for i in range(4):\n",
    "    images.append(T.RandomResizedCrop(RESIZE, scale=(.6, 1.0))(img))\n",
    "show(images)\n",
    "\n",
    "images = [img]\n",
    "for i in range(4):\n",
    "    images.append(T.RandomAffine(15, fill=1.)(img))\n",
    "show(images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OmniglotTrain(Dataset):\n",
    "    \"\"\"A customized dataset that returns a pair. The support label matches the\n",
    "    query label based on the rate parameter `r`.\n",
    "    \"\"\"\n",
    "    def __init__(self, dataset, r=0.5):\n",
    "        super(OmniglotTrain, self).__init__()\n",
    "\n",
    "        self.dataset = dataset\n",
    "        self.r = r\n",
    "        self.load_lookup()\n",
    "\n",
    "    def load_lookup(self):\n",
    "        \"\"\"Build positive and negative indices lookup table for all classes\"\"\"\n",
    "        self.lookup = {}\n",
    "        labels = [sample[1] for sample in self.dataset]\n",
    "        unique_labels = np.unique(labels)\n",
    "        for lbl in unique_labels:\n",
    "            indices_pos = [i for i, _lbl in enumerate(labels) if _lbl == lbl]\n",
    "            indices_neg = [i for i in range(len(self)) if i not in indices_pos]\n",
    "            self.lookup[lbl] = (indices_pos, indices_neg)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        draw = torch.rand(1).item()\n",
    "        query = self.dataset[index]\n",
    "        lbl_true = query[1]\n",
    "        if draw <= self.r:  # Return a positive pair\n",
    "            idx_support = np.random.choice([i for i in self.lookup[lbl_true][0] if i != index], 1)[0]\n",
    "            output = 1  # If match return 1, else return 0.\n",
    "        else:  # Return a negative pair\n",
    "            idx_support = np.random.choice(self.lookup[lbl_true][1], 1)[0]\n",
    "            output = 0\n",
    "        support = self.dataset[idx_support]\n",
    "        return query[0], support[0], torch.Tensor([output]), (query[1], support[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OmniglotQuery(Dataset):\n",
    "    \"\"\"A customized dataset that returns 20-way 1-shot query and support tuple.\n",
    "    \"\"\"\n",
    "    def __init__(self, dataset, num_ways=20):\n",
    "        super(OmniglotQuery, self).__init__()\n",
    "\n",
    "        self.dataset = dataset\n",
    "        self.num_ways = num_ways\n",
    "        self.load_lookup()\n",
    "\n",
    "    def load_lookup(self):\n",
    "        \"\"\"Build positive and negative indices lookup table for all classes\n",
    "        self.lookup[label][0]: indices of examples with matching the label.\n",
    "        self.lookup[label][1]: indices of any other examples.\n",
    "        \"\"\"\n",
    "        self.lookup = {}\n",
    "        labels = [sample[1] for sample in self.dataset]\n",
    "        unique_labels = np.unique(labels)\n",
    "        for lbl in unique_labels:\n",
    "            indices_pos = [i for i, _lbl in enumerate(labels) if _lbl == lbl]\n",
    "            indices_neg = [i for i in range(len(self)) if i not in indices_pos]\n",
    "            self.lookup[lbl] = (indices_pos, indices_neg)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        \"\"\"Compute support that only contains 1 aligned example. \"\"\"\n",
    "        query = self.dataset[index]\n",
    "        lbl_true = query[1]\n",
    "        idx_match = np.random.choice([i for i in self.lookup[lbl_true][0] if i != index], 1)\n",
    "        # The rest of examples should NOT match the true label\n",
    "        idx_support = np.random.choice(self.lookup[lbl_true][1], self.num_ways - 1)\n",
    "        # Combine 1 matched and rest unmatched indices\n",
    "        indices = np.random.permutation(np.concatenate([idx_match, idx_support]))\n",
    "        assert len(indices) == self.num_ways, 'Support does not match n_ways!'\n",
    "        support_x = torch.stack([self.dataset[i][0] for i in indices])\n",
    "        support_y = torch.LongTensor([self.dataset[i][1] for i in indices])\n",
    "        # Return: x_query, x_supports, y_query, y_supports\n",
    "        return query[0], support_x, query[1], support_y\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_train = OmniglotTrain(omniglot_bg)\n",
    "loader_train = DataLoader(dataset_train, batch_size=BATCH_SIZE, shuffle=True, num_workers=NUM_WORKERS)\n",
    "\n",
    "dataset_test = OmniglotTrain(omniglot_eval)\n",
    "loader_test = DataLoader(dataset_test, batch_size=BATCH_SIZE, shuffle=False, num_workers=NUM_WORKERS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OmniglotModule(pl.LightningModule):\n",
    "    def __init__(self, model, lr=0.001):\n",
    "        super(OmniglotModule, self).__init__()\n",
    "        self.model = model\n",
    "        self.loss_fn = torch.nn.BCEWithLogitsLoss()\n",
    "        self.save_hyperparameters(ignore=['model'])  # Do not save the entire model!\n",
    "\n",
    "    def forward(self, x1, x2):\n",
    "        return self.model(x1, x2)\n",
    "    \n",
    "    def configure_optimizers(self):\n",
    "        # optimizer = torch.optim.SGD(\n",
    "        #     self.model.parameters(),\n",
    "        #     lr=self.hparams.lr,\n",
    "        #     momentum=0.9,\n",
    "        # )\n",
    "        optimizer = torch.optim.Adam(self.model.parameters(), lr=self.hparams.lr)\n",
    "        return optimizer\n",
    "    \n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x1 , x2, match, _ = batch\n",
    "        output = self(x1, x2)\n",
    "        loss = self.loss_fn(output, match)\n",
    "        self.log('train_loss', loss, on_step=False, on_epoch=True, prog_bar=True)\n",
    "        return loss\n",
    "        \n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        loss = self.__evaluate(batch)\n",
    "        self.log('validation_loss', loss, on_step=False, on_epoch=True, prog_bar=True)\n",
    "        return loss\n",
    "\n",
    "    def test_step(self, batch, batch_idx):\n",
    "        loss = self.__evaluate(batch)\n",
    "        self.log('test_loss', loss, on_step=False, on_epoch=True, prog_bar=True)\n",
    "        return loss\n",
    "\n",
    "    def __evaluate(self, batch):\n",
    "        x1 , x2, match, _ = batch\n",
    "        output = self(x1, x2)\n",
    "        loss = self.loss_fn(output, match)\n",
    "        return loss\n",
    "    \n",
    "    def predict_step(self, batch, batch_idx):\n",
    "        x_query, x_supports, y_query, y_supports = batch\n",
    "        batch_size = x_query.size(0)\n",
    "        support_size = x_supports.size(1)\n",
    "          # Set everything to a dummy label `-1`\n",
    "        predictions = -torch.ones(batch_size).to(torch.long).to(self.device)\n",
    "        for i in range(batch_size):\n",
    "            query = x_query[i]\n",
    "            repeated_query = query.repeat(support_size, 1, 1, 1)\n",
    "            outputs = F.sigmoid(self(repeated_query, x_supports[i]))\n",
    "            pred_idx = torch.argmax(outputs)  # Find support\n",
    "            pred_lbl = y_supports[i][pred_idx]  # Find support's label\n",
    "            predictions[i] = pred_lbl\n",
    "        # Combine prediction and true labels as column vectors [pred, true]\n",
    "        return torch.stack([predictions, y_query]).T\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load pre-trained model\n",
    "# VISION = '0'\n",
    "# CHECKPOINT_FILE = 'epoch=49-step=10050.ckpt\n",
    "# PATH_CHECKPOINT = os.path.join(os.getcwd(), 'lightning_logs', f'version_{VISION}', 'checkpoints', CHECKPOINT_FILE)\n",
    "\n",
    "# net = Siamese()\n",
    "# if os.path.exists(PATH_CHECKPOINT):\n",
    "#     siamese_net = OmniglotModule.load_from_checkpoint(PATH_CHECKPOINT, model=net)\n",
    "\n",
    "# trainer = pl.Trainer(accelerator='gpu', enable_progress_bar=True, precision='16-mixed')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using 16bit Automatic Mixed Precision (AMP)\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/lukec/workspace/pygeo_playground/venv/lib/python3.9/site-packages/pytorch_lightning/trainer/connectors/logger_connector/logger_connector.py:67: UserWarning: Starting from v1.9.0, `tensorboardX` has been removed as a dependency of the `pytorch_lightning` package, due to potential conflicts with other packages in the ML ecosystem. For this reason, `logger=True` will use `CSVLogger` as the default logger, unless the `tensorboard` or `tensorboardX` packages are found. Please `pip install lightning[extra]` or one of them to enable TensorBoard support by default\n",
      "  warning_cache.warn(\n",
      "You are using a CUDA device ('NVIDIA GeForce RTX 3070') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name    | Type              | Params\n",
      "----------------------------------------------\n",
      "0 | model   | Siamese           | 34.2 M\n",
      "1 | loss_fn | BCEWithLogitsLoss | 0     \n",
      "----------------------------------------------\n",
      "34.2 M    Trainable params\n",
      "0         Non-trainable params\n",
      "34.2 M    Total params\n",
      "136.722   Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 49: 100%|██████████| 201/201 [00:08<00:00, 24.03it/s, v_num=0, validation_loss=0.191, train_loss=0.147]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=50` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 49: 100%|██████████| 201/201 [00:08<00:00, 22.59it/s, v_num=0, validation_loss=0.191, train_loss=0.147]\n"
     ]
    }
   ],
   "source": [
    "# Train model\n",
    "MAX_EPOCH = 50\n",
    "\n",
    "net = Siamese()\n",
    "siamese_net = OmniglotModule(net)\n",
    "trainer = pl.Trainer(max_epochs=MAX_EPOCH, accelerator='gpu', enable_progress_bar=True, precision='16-mixed')\n",
    "trainer.fit(siamese_net, loader_train, loader_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_query = OmniglotQuery(omniglot_eval)\n",
    "loader_query = DataLoader(dataset_query, batch_size=100, shuffle=False, num_workers=NUM_WORKERS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting DataLoader 0: 100%|██████████| 132/132 [00:23<00:00,  5.74it/s]\n"
     ]
    }
   ],
   "source": [
    "predictions = torch.concat(trainer.predict(siamese_net, dataloaders=loader_query))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(predictions):\n",
    "    \"\"\"Compute accuracy\"\"\"\n",
    "    pred = predictions[:, 0]  # 1st column\n",
    "    y_true = predictions[:, 1]  # 2nd column\n",
    "    return torch.sum(pred == y_true) / pred.size(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on 20way-1shot is: 79.70\n"
     ]
    }
   ],
   "source": [
    "acc = accuracy(predictions)\n",
    "\n",
    "print(f'Accuracy on 20way-1shot is: {acc*100:.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
